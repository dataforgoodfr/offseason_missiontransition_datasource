import json
from ast import literal_eval
from os.path import join
from typing import Optional

import pandas as pd

from scripts.clean_pdf import get_clean_text
from scripts.read_pdf_files import get_pdf_content_from_url

# directory to csv generated by scrap_pdf_files.py


def compute_pdf_content(
        directory: str,
        csv: str,
        nb_samples: Optional[int] = None,
        debug_print: bool = False
):

    path_to_csv = join(directory, csv)
    data = pd.read_csv(path_to_csv)

    data_with_pdf: pd.DataFrame = data[data['pdfs'] != '[]']

    sample_data: pd.DataFrame
    if nb_samples is None:
        sample_data = data_with_pdf
    else:
        sample_data = data_with_pdf.sample(nb_samples)

    for idx, row in sample_data.iterrows():

        aide_name = row['name']  # nom de l'aide
        pdfs_list = literal_eval(row['pdfs'])  # liste des urls des pdfs associés à l'aide

        content_dict = {}
        for url in pdfs_list:
            content = get_pdf_content_from_url(url)
            content_dict[url] = get_clean_text(
                input_text=content,
                debug_print=debug_print
            )

        path = join(directory, aide_name + '.json')
        with open(path, "w") as f:
            json.dump(content_dict, f)
